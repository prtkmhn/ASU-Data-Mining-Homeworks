# -*- coding: utf-8 -*-
"""DataMiningHomework2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R14JTzGlqfB4q_k2etAakhEN3THb-yD2
"""

!pip install rake-nltk

from google.colab import drive
drive.mount('/content/drive')
import numpy as np
import random
from tqdm import tqdm
import pandas as pd
import string
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import *

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

import matplotlib.pyplot as plt
import gensim.downloader as api
import gensim

train_data_path = '/content/drive/MyDrive/Colab Notebooks/Homework2/24_train_1.csv'
test_data_path = '/content/drive/MyDrive/Colab Notebooks/Homework2/news-test.csv'

# Load data
train_data = pd.read_csv(train_data_path)
test_data = pd.read_csv(test_data_path)

# Preprocessing function
def preprocess_text(text):
    # Tokenization
    tokens = nltk.word_tokenize(text)

    # Lowercasing
    tokens = [w.lower() for w in tokens]

    # Removing punctuation
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]

    # Removing non-alphabetic tokens
    words = [word for word in stripped if word.isalpha()]

    # Removing stopwords
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if not w in stop_words]

    # Stemming
    porter = PorterStemmer()
    stemmed = [porter.stem(word) for word in words]

    return ' '.join(stemmed)

# Apply preprocessing to training data
train_data['Processed_Text'] = train_data['Text'].apply(preprocess_text)

# Apply preprocessing to test data
test_data['Processed_Text'] = test_data['Text'].apply(preprocess_text)

# Create n-grams
def generate_ngrams(text, n_gram=1):
    token = [token for token in text.split(" ") if token != "" if token not in stopwords.words('english')]
    ngrams = zip(*[token[i:] for i in range(n_gram)])
    return [" ".join(ngram) for ngram in ngrams]

# Generate n-grams for training data
train_data['Processed_Text_ngrams'] = train_data['Processed_Text'].apply(lambda x: generate_ngrams(x, n_gram=2))

# Generate n-grams for test data
test_data['Processed_Text_ngrams'] = test_data['Processed_Text'].apply(lambda x: generate_ngrams(x, n_gram=2))

# Load GloVe embeddings
glove_vectors = api.load("glove-twitter-100")
glove_embeddings = []

for doc in train_data['Processed_Text_ngrams']:
    doc_embedding = []
    for word in doc:
        if word in glove_vectors:
            doc_embedding.append(glove_vectors[word])
    if doc_embedding:
        doc_embedding = np.mean(doc_embedding, axis=0)
    else:
        doc_embedding = np.zeros(100)
    glove_embeddings.append(doc_embedding)

glove_embeddings = np.array(glove_embeddings)

# Create TF-IDF features
tfidf = TfidfVectorizer(ngram_range=(1,2))
tfidf_train = tfidf.fit_transform(train_data['Processed_Text_ngrams'].apply(lambda x: ' '.join(x)))
tfidf_test = tfidf.transform(test_data['Processed_Text_ngrams'].apply(lambda x: ' '.join(x)))

# Train Word2Vec model
w2v_model = gensim.models.Word2Vec(train_data['Processed_Text_ngrams'], vector_size=100, window=5, min_count=1, workers=4)

w2v_embeddings = []
for doc in train_data['Processed_Text_ngrams']:
    doc_embedding = []
    for word in doc:
        if word in w2v_model.wv:
            doc_embedding.append(w2v_model.wv[word])
    if doc_embedding:
        doc_embedding = np.mean(doc_embedding, axis=0)
    else:
        doc_embedding = np.zeros(100)
    w2v_embeddings.append(doc_embedding)

w2v_embeddings = np.array(w2v_embeddings)

# Define neural network model
class NewsClassifier(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, output_dim):
        super(NewsClassifier, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.relu(out)
        out = self.fc3(out)
        return out

# Define training function
def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    running_corrects = 0
    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        _, preds = torch.max(outputs, 1)
        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)
        loss.backward()
        optimizer.step()
    epoch_loss = running_loss / len(train_loader.dataset)
    epoch_acc = running_corrects.double() / len(train_loader.dataset)
    return epoch_loss, epoch_acc

# Define evaluation function
def eval_model(model, test_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    running_corrects = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            _, preds = torch.max(outputs, 1)
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)
    epoch_loss = running_loss / len(test_loader.dataset)
    epoch_acc = running_corrects.double() / len(test_loader.dataset)
    return epoch_loss, epoch_acc