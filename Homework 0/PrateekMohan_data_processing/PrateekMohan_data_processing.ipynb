{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install required packages and nltk packages\n"
      ],
      "metadata": {
        "id": "F1AHjgAXEchV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "import math\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyVHU8YaY3U6",
        "outputId": "8b661f7d-8f9d-48f6-dc6f-ae9c0ac1ee8d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset and Load the predefined dictionary\n"
      ],
      "metadata": {
        "id": "V0q6O0wqcifH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9bwTIrp8YdAx"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('24_train_1.csv')  # Replace '24_train_1.csv' with the actual file path\n",
        "\n",
        "# Stemming tool from nltk\n",
        "stemmer = PorterStemmer()\n",
        "# A mapping dictionary that helps remove punctuations\n",
        "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to preprocess text\n"
      ],
      "metadata": {
        "id": "h9w-R_YocnFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Turn document into lowercase\n",
        "    lowers = text.lower()\n",
        "    # Remove punctuations\n",
        "    no_punctuation = lowers.translate(remove_punctuation_map)\n",
        "    # Tokenize document\n",
        "    tokens = nltk.word_tokenize(no_punctuation)\n",
        "    # Remove stop words\n",
        "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
        "    # Stemming process\n",
        "    stemmed = [stemmer.stem(item) for item in filtered]\n",
        "    # Final unigrams\n",
        "    return stemmed"
      ],
      "metadata": {
        "id": "b_P4EuiZcmZU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply text preprocessing to each document in the 'Text' column\n",
        "df['Processed_Text'] = df['Text'].apply(preprocess_text)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaJmSi2YZUsO",
        "outputId": "0524fa6f-e028-4af4-b5bd-cd01a7c6c361"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ArticleId                                               Text  \\\n",
            "0         1429  sfa awaits report over mikoliunas the scottish...   \n",
            "1         1896  parmalat to return to stockmarket parmalat  th...   \n",
            "2         1633  edu blasts arsenal arsenal s brazilian midfiel...   \n",
            "3         2178  henman decides to quit davis cup tim henman ha...   \n",
            "4          194  french suitor holds lse meeting european stock...   \n",
            "..         ...                                                ...   \n",
            "995       1250  blair  damaged  by blunkett row a majority of ...   \n",
            "996       1639  a november to remember last saturday  one news...   \n",
            "997        916  highbury tunnel players in clear the football ...   \n",
            "998       2217  top stars join us tsunami tv show brad pitt  r...   \n",
            "999        902  eastwood s baby scoops top oscars clint eastwo...   \n",
            "\n",
            "          Category                                     Processed_Text  \n",
            "0            sport  [sfa, await, report, mikoliuna, scottish, foot...  \n",
            "1         business  [parmalat, return, stockmarket, parmalat, ital...  \n",
            "2            sport  [edu, blast, arsen, arsen, brazilian, midfield...  \n",
            "3            sport  [henman, decid, quit, davi, cup, tim, henman, ...  \n",
            "4         business  [french, suitor, hold, lse, meet, european, st...  \n",
            "..             ...                                                ...  \n",
            "995       politics  [blair, damag, blunkett, row, major, voter, 68...  \n",
            "996          sport  [novemb, rememb, last, saturday, one, newspap,...  \n",
            "997          sport  [highburi, tunnel, player, clear, footbal, ass...  \n",
            "998  entertainment  [top, star, join, us, tsunami, tv, show, brad,...  \n",
            "999  entertainment  [eastwood, babi, scoop, top, oscar, clint, eas...  \n",
            "\n",
            "[1000 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate unigrams for each document"
      ],
      "metadata": {
        "id": "jbOBEZ9GdBA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dictionary\n",
        "with open('dictionary.txt', 'r') as file:\n",
        "    dictionary = set(line.strip() for line in file)\n",
        "\n",
        "# Function to filter unigrams based on the dictionary\n",
        "def filter_unigrams(unigrams):\n",
        "    return [word for word in unigrams if word in dictionary]\n",
        "\n",
        "# Apply the filtering function to each row in the 'Processed_Text' column\n",
        "df['Filtered_Unigrams'] = df['Processed_Text'].apply(filter_unigrams)\n",
        "\n",
        "# Now, df['Filtered_Unigrams'] contains the filtered unigrams for each document\n",
        "print(df['Filtered_Unigrams'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMgi2jdtZj7n",
        "outputId": "8e978fbd-bdaf-4508-df29-3fbe1f5499d6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      [report, scottish, footbal, associ, refere, re...\n",
            "1      [return, compani, went, account, hope, back, s...\n",
            "2      [arsen, arsen, hit, club, offer, new, contract...\n",
            "3      [decid, quit, davi, cup, great, britain, davi,...\n",
            "4      [french, hold, meet, european, stock, market, ...\n",
            "                             ...                        \n",
            "995    [blair, damag, row, major, voter, believ, prim...\n",
            "996    [novemb, last, saturday, one, newspap, england...\n",
            "997    [player, clear, footbal, associ, said, bring, ...\n",
            "998    [top, star, join, us, tsunami, tv, show, rober...\n",
            "999    [top, oscar, million, dollar, beat, martin, av...\n",
            "Name: Filtered_Unigrams, Length: 1000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame with relevant columns\n",
        "new_df = df[['ArticleId', 'Filtered_Unigrams', 'Category']]\n",
        "\n",
        "# Save the new DataFrame as a CSV file\n",
        "new_df.to_csv('processed_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "b9aeKB7Jb5a4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2 TFIDF Matrix Calculation"
      ],
      "metadata": {
        "id": "mirX7ujUcNPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate Term Frequency (TF) for each document\n",
        "def calculate_tf(document):\n",
        "    word_count = Counter(document)\n",
        "    max_occurrences = max(word_count.values())\n",
        "    tf = {word: count / max_occurrences for word, count in word_count.items()}\n",
        "    return tf\n",
        "\n",
        "# Function to calculate Inverse Document Frequency (IDF) for each word\n",
        "def calculate_idf(documents):\n",
        "    document_count = len(documents)\n",
        "    idf = {word: math.log(document_count / sum(1 for doc in documents if word in doc))\n",
        "           for word in set(word for doc in documents for word in doc)}\n",
        "    return idf\n",
        "\n",
        "# Function to calculate TFIDF for each document and word\n",
        "def calculate_tfidf(documents, idf):\n",
        "    tfidf_matrix = []\n",
        "    for document in documents:\n",
        "        tf = calculate_tf(document)\n",
        "        tfidf = {word: tf[word] * idf[word] for word in tf.keys()}\n",
        "        tfidf_matrix.append(tfidf)\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Apply TFIDF calculation to the 'Filtered_Unigrams' column\n",
        "idf_values = calculate_idf(df['Filtered_Unigrams'])\n",
        "tfidf_matrix = calculate_tfidf(df['Filtered_Unigrams'], idf_values)\n",
        "\n",
        "with open('matrix.txt', 'w', encoding='utf-8') as matrix_file:\n",
        "    for tfidf_scores in tfidf_matrix:\n",
        "        line = ','.join(str(tfidf_scores.get(word, 0)) for word in dictionary)\n",
        "        matrix_file.write(line + '\\n')"
      ],
      "metadata": {
        "id": "PNgdTbJ3d3c8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compute Top 3 Most Frequent Words:"
      ],
      "metadata": {
        "id": "ibJN_uO8eql5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute the top 3 most frequent words for each category\n",
        "def top_most_frequent_words_per_category(df):\n",
        "    top_frequency = {}\n",
        "    for category in df['Category'].unique():\n",
        "        category_df = df[df['Category'] == category]\n",
        "        all_words = [word for sublist in category_df['Filtered_Unigrams'] for word in sublist]\n",
        "        word_counts = Counter(all_words)\n",
        "        top_frequency[category] = dict(word_counts.most_common(3))\n",
        "\n",
        "    # Save results to 'frequency.json'\n",
        "    with open('frequency.json', 'w') as frequency_file:\n",
        "        json.dump(top_frequency, frequency_file, indent=4)\n",
        "\n",
        "# Call the function to calculate and save the top 3 most frequent words\n",
        "top_most_frequent_words_per_category(df)"
      ],
      "metadata": {
        "id": "kwk3HgO8eq82"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compute Top 3 Highest Average TFIDF Words:\n"
      ],
      "metadata": {
        "id": "gM6l02w0ewOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute the top 3 highest average TFIDF words for each category\n",
        "def top_average_tfidf_per_category(df, tfidf_matrix):\n",
        "    top_tfidf = {}\n",
        "    for category in df['Category'].unique():\n",
        "        category_df = df[df['Category'] == category]\n",
        "        category_tfidf_matrix = [tfidf_matrix[i] for i in category_df.index]\n",
        "\n",
        "        # Filter out words that are not present in any document in the category\n",
        "        unique_words = set(word for doc in category_tfidf_matrix for word in doc.keys())\n",
        "\n",
        "        average_tfidf = {}\n",
        "        for word in unique_words:\n",
        "            total_tfidf = sum(doc.get(word, 0) for doc in category_tfidf_matrix)\n",
        "            average_tfidf[word] = total_tfidf / len(category_tfidf_matrix)\n",
        "\n",
        "        top_tfidf[category] = dict(sorted(average_tfidf.items(), key=lambda x: x[1], reverse=True)[:3])\n",
        "\n",
        "    # Save results to 'scores.json'\n",
        "    with open('scores.json', 'w') as scores_file:\n",
        "        json.dump(top_tfidf, scores_file, indent=4)\n",
        "\n",
        "# Call the function to calculate and save the top 3 highest average TFIDF words\n",
        "top_average_tfidf_per_category(df, tfidf_matrix)\n"
      ],
      "metadata": {
        "id": "tgduFbJ-ewYQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwma_E1LExys"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}